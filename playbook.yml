---
- hosts: localhost
  gather_facts: no
  tasks:
    - name: Check if WireGuard is running
      become: yes
      shell: "wg show | grep -q interface"
      register: wg_status
      ignore_errors: true

    - name: Bring WireGuard VPN up with wg_setup.sh if WireGuard is not running
      become: yes
      shell: sh wg_setup.sh
      args:
        chdir: "{{ playbook_dir }}"
      when: wg_status.failed

    - name: Ensure VM ssh ports are available before proceeding
      shell: sh check_ports.sh
      args:
        chdir: "{{ playbook_dir }}"
      register: port_check_result
      failed_when: port_check_result.rc != 0

    - name: Check if the private SSH key for temp cluster exists
      stat:
        path: "{{ playbook_dir }}/ansible_key"
      register: ssh_private_key

    - name: Generate new SSH key pair if it does not exist
      command: ssh-keygen -t rsa -b 2048 -f {{ playbook_dir }}/ansible_key -N ""
      when: not ssh_private_key.stat.exists

    - name: Ensure Vagrant is up
      shell: vagrant up
      args:
        chdir: "{{ playbook_dir }}"
      register: vagrant_up_result
      ignore_errors: yes  # Continue even if Vagrant is already up

    - name: Display Vagrant up result
      debug:
        var: vagrant_up_result.stdout_lines

    - name: Check the status of Vagrant VMs
      shell: vagrant status
      args:
        chdir: "{{ playbook_dir }}"  # Ensures the command runs in the directory containing the Vagrantfile
      register: vagrant_status
      ignore_errors: yes

    - name: Display Vagrant status
      debug:
        var: vagrant_status.stdout_lines

    - name: Show the WireGuard VPN
      become: yes
      shell: "wg show"
      register: wg_status

    - name: Display Vagrant status
      debug:
        var: wg_status.stdout_lines

#########################        
# Lets setup the VMs now!
#########################        
- hosts: all
  become: yes
  tasks:
    - name: Install required packages
      zypper:
        name:
          - curl
          - wget
          - vim
          - docker
          - zip
          - unzip
          - bind-utils
        state: present

- hosts: k3s
  become: yes
  tasks:
    - name: Get IP address of the wg0 interface
      shell: ip -4 addr show wg0 | grep -oP '(?<=inet\s)\d+(\.\d+){3}'
      register: wg0_ip

    - name: Set node_ip fact
      set_fact:
        node_ip: "{{ wg0_ip.stdout }}"

    - name: Install k3s on k3s node
      shell: curl -sfL https://get.k3s.io | sh -s - --cluster-cidr=10.244.0.0/16
      register: k3s_install_result
      retries: 5         # Number of retries
      delay: 30          # Wait 30 seconds between retries
      until: k3s_install_result.rc == 0

    - name: Generate k3s configuration file
      template:
        src: templates/k3s-config.yaml.j2  # Adjust this path to your actual template location if different
        dest: /etc/rancher/k3s/config.yaml
        mode: '0644'

    - name: Reload systemd to apply k3s changes
      command: systemctl daemon-reload

    - name: Restart k3s service
      systemd:
        name: k3s
        state: restarted
        enabled: yes

    - name: Install kubectl in k3s for testing
      shell: |
        curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/arm64/kubectl"
        chmod +x kubectl
        mv kubectl /usr/local/bin/

    - name: Fetch kubeconfig directly to local machine
      fetch:
        src: /etc/rancher/k3s/k3s.yaml
        dest: ./k3s-{{ inventory_hostname }}.yaml
        flat: yes
        validate_checksum: no

    - name: Install Flannel CNI
      shell: |
        KUBECONFIG=/etc/rancher/k3s/k3s.yaml /usr/local/bin/kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

    - name: Render the CoreDNS ConfigMap template on the node
      template:
        src: templates/coredns_cm.conf.j2
        dest: /tmp/coredns_cm.conf

    - name: Apply the CoreDNS ConfigMap using kubectl from within the cluster
      shell: |
        KUBECONFIG=/etc/rancher/k3s/k3s.yaml /usr/local/bin/kubectl apply -f /tmp/coredns_cm.conf

    - name: Modify CoreDNS service to use LoadBalancer type
      shell: |
        KUBECONFIG=/etc/rancher/k3s/k3s.yaml  /usr/local/bin/kubectl patch svc kube-dns -n kube-system -p '{"spec": {"type": "LoadBalancer"}}'
      register: patch_coredns

    - name: Restart CoreDNS deployment using kubectl from within the cluster
      shell: |
        KUBECONFIG=/etc/rancher/k3s/k3s.yaml /usr/local/bin/kubectl -n kube-system rollout restart deployment coredns

    - name: Wait until CoreDNS pods are running and ready
      shell: |
        KUBECONFIG=/etc/rancher/k3s/k3s.yaml /usr/local/bin/kubectl -n kube-system wait --for=condition=Ready pod -l k8s-app=kube-dns --timeout=300s
      register: wait_result
      retries: 6
      delay: 10
      until: wait_result.rc == 0
      ignore_errors: yes

    - name: Deploy resolv.conf template
      template:
        src: templates/resolv.conf.j2
        dest: /etc/resolv.conf
        owner: root
        group: root
        mode: '0644'

###############################################
# Lets move to setup all of the Nomad stuff now
###############################################
- hosts: nomad_server
  become: yes
  tasks:
    - name: Install Nomad on server node
      get_url:
        url: https://releases.hashicorp.com/nomad/1.8.2/nomad_1.8.2_linux_arm64.zip
        dest: /tmp/nomad.zip

    - name: Unzip and install Nomad on server
      unarchive:
        src: /tmp/nomad.zip
        dest: /usr/local/bin/
        remote_src: yes

    - name: Create Nomad configuration directory on server
      file:
        path: /etc/nomad.d
        state: directory
        mode: '0755'

    - name: Create Nomad certificates directory on server
      file:
        path: /etc/nomad.d/nomad_certificates
        state: directory
        mode: '0755'

    - name: Copy all Nomad certificates to server
      copy:
        src: "{{ item }}"
        dest: "/etc/nomad.d/nomad_certificates/"
        mode: '0644'
      with_fileglob:
        - "nomad_certificates/*.pem"

    - name: Generate Nomad server configuration file from template
      template:
        src: templates/server.hcl.j2
        dest: /etc/nomad.d/nomad.hcl
        mode: '0644'

    - name: Configure and start Nomad service on server
      template:
        src: templates/nomad.service.j2
        dest: /etc/systemd/system/nomad.service

    - name: Reload systemd to apply Nomad changes on server
      command: systemctl daemon-reload

    - name: Start and enable Nomad service on server
      systemd:
        name: nomad
        state: started
        enabled: yes

    - name: Wait for Nomad server to be ready
      wait_for:
        port: 4646
        host: "{{ inventory_hostname }}"
        delay: 10
        timeout: 300

    - name: Generate the bootstrap management token
      command: /usr/local/bin/nomad acl bootstrap -json
      register: bootstrap_token

    - name: Set the bootstrap token as a fact
      set_fact:
        nomad_bootstrap_token: "{{ bootstrap_token.stdout | from_json }}"

    - name: Store the bootstrap management token locally
      delegate_to: localhost
      copy:
        content: "{{ bootstrap_token.stdout }}"
        dest: ./nomad_bootstrap_token.json
      become: no  # Make sure sudo is not used

